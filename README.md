# ğŸª Medical Chatbot A.I+

A Flask-based medical chatbot application powered by LLMs (e.g., Llama 2).  
This project allows users to ask health-related questions and receive intelligent AI-generated responses.

---

## ğŸ“‚ Project Structure

```
ğŸ–Œï¸ app.py                # Main Flask application
ğŸ“‚ model_files/                # Pretrained LLM model binaries (e.g., .bin, model.txt)
ğŸ“‚ src/
ğŸ“Œ db/               # Database helpers (e.g., Pinecone index management)
ğŸ“Œ helper/           # Utility modules (config management, HuggingFace API helpers)
ğŸ“Œ model/            # Model loading and Retrieval-Augmented Generation (RAG) logic
ğŸ“Œ static/           # Frontend CSS styles
ğŸ“Œ templates/        # HTML templates (chat UI)

ğŸ–Œï¸ requirements.txt      # Python dependencies
ğŸ–Œï¸ README.md             # Project documentation (this file)
```
## ğŸ‘… Download the Llama 2 Model
We are using **Llama 2 7B Chat** model (compressed GGML format).

ğŸ‘‰ You can download the model from [Hugging Face](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main)

Recommended file:
```
llama-2-7b-chat.ggmlv3.q4_0.bin
```
Save the model inside the `model/` directory.

> **Note**: You can use a different GGML model version depending on your hardware.

---

## ğŸš€ How to Run

1. **Clone the repo**
   ```bash
   git clone https://github.com/fury-r/medical-chatbot-rag.git
   cd medical-chatbot-rag
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Setup Environment Variables**

   Create a `.env` file or export variables:

   ```env
   HUGGINGFACE_API_TOKEN=your_huggingface_token
   PINECONE_API_KEY=your_pinecone_key
   PINECONE_ENV=your_pinecone_environment
   PINECONE_INDEX_NAME=your_index_name
   HUGGING_FACE_MODEL_NAME=name_or_id_of_model
   MODEL_PATH=path/to/your/.bin/file
   MODEL_TYPE=
   MODEL_TOKEN_LIMIT=
   MODEL_TEMPERATURE=
   MODEL_MEMORY= //optonal
   ```

4. **Start the app**
   ```bash
   python app.py
   ```

5. Open [http://localhost:5000](http://localhost:5000) in your browser ğŸ¯

---

## ğŸ’… Features

- ğŸ¥ Medical domain-focused chatbot
- ğŸ›¼ Retrieval-Augmented Generation (RAG) supported
- ğŸ’¬ Modern Chat UI (dark mode)
- ğŸ”— Integrated with HuggingFace Inference API (optional)
- ğŸ” Pinecone Vector Search for context retrieval
- ğŸ“¦ Local model serving (with `.bin` weights)

---


---

## ğŸ› ï¸ Tech Stack

- **Backend:** Flask
- **Frontend:** Vanilla JavaScript + HTML/CSS
- **LLM Models:** Llama 2, HuggingFace models
- **Vector Database:** Pinecone
- **Model Serving:** Local or Remote

---

## âš™ï¸ Configuration

You can manage all environment variables and model paths using the `Config` class in `src/helper/config.py`.

---

## âœ¨ Future Improvements

- User Authentication (Login/Signup)
- Chat History Saving (DB Storage)
- Better Medical Disclaimer System
- Real-time Typing Indicators

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).
